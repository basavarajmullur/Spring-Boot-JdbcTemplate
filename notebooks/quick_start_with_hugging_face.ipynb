{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fgVWTMK9SNz"
      },
      "source": [
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q fastapi uvicorn pyngrok pillow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xt2XZgaaH2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you'll need to have a runtime with [sufficient resources](https://ai.google.dev/gemma/docs/core#sizes) to run the MedGemma model.\n",
        "\n",
        "You can try out MedGemma 4B for free in Google Colab using a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **‚ñæ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "**Note**: To run the demo with MedGemma 27B in Google Colab, you will need a runtime with an A100 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ITcQtdal7J"
      },
      "source": [
        "### Get access to MedGemma\n",
        "\n",
        "Before you get started, make sure that you have access to MedGemma models on Hugging Face:\n",
        "\n",
        "1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).\n",
        "2. Head over to the [MedGemma model page](https://huggingface.co/google/medgemma-1.5-4b-it) and accept the usage conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFQnPL2a9Dj"
      },
      "source": [
        "### Step 1: Authenticate with Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZwUUIY0gpY4W"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7xTbWg6pY4W"
      },
      "source": [
        "### Step 2: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CulOXOrhpY4W"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "  fastapi \\\n",
        "  uvicorn \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  pillow==10.4.0 \\\n",
        "  torch torchvision \\\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRN9Yg_kpY4X"
      },
      "source": [
        "## Step 3: Load MedGemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YORs_sDfpY4X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "431f2487eeb74f97836d69cf20c96c74",
            "46dccbc89bdf45cf887ee32e4dc3700b",
            "62a3ddec75154d2098465a6ef602688f",
            "5b5a7f7c46ce423d9db58c6d197e23aa",
            "c0658eb476f849f79c9e71cf1a2653c3",
            "689e112d2c984bfe878daa5d4ceca102",
            "7a7d1e2e00f14287bc7e8f9f9b4a6473",
            "4bbb1c054b264bc292d683ac3c7390d4",
            "fc1d5a6174a142cbad45ea9dd68a9628",
            "9e45926cb57547c4ab49962eae23586c",
            "707cba7658904aa29912fca9f7e78452"
          ]
        },
        "outputId": "d108bdf5-c927-420a-c0d7-8df625b79090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The image processor of type `Gemma3ImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "431f2487eeb74f97836d69cf20c96c74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ MedGemma loaded\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "\n",
        "MODEL_ID = \"google/medgemma-4b-it\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ MedGemma loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPhEFjiOTpcM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2Of7LKuT_Sz"
      },
      "source": [
        "## Step 4: Install cloudflared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dh1QcEXJT8zj"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9F5HEr6UMqO"
      },
      "source": [
        "## Step 5: MEDICAL SYSTEM PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FjGwhqdfUVI0"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, UploadFile, Form\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "import base64\n",
        "from fastapi import HTTPException\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import traceback\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "def base64_to_pil(image_base64: str) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Convert a base64 string (raw or data URL) into a PIL Image.\n",
        "\n",
        "    Supports:\n",
        "    - Raw base64 (no prefix)\n",
        "    - data:image/png;base64,...\n",
        "    - data:image/jpeg;base64,...\n",
        "    \"\"\"\n",
        "\n",
        "    if not image_base64:\n",
        "        raise ValueError(\"Empty base64 image string\")\n",
        "\n",
        "    # Strip data URL prefix if present\n",
        "    if image_base64.startswith(\"data:\"):\n",
        "        image_base64 = image_base64.split(\",\", 1)[1]\n",
        "\n",
        "    try:\n",
        "        image_bytes = base64.b64decode(image_base64, validate=True)\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"Invalid base64 image data\") from e\n",
        "\n",
        "    try:\n",
        "        image = Image.open(io.BytesIO(image_bytes))\n",
        "        image = image.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"Decoded bytes are not a valid image\") from e\n",
        "\n",
        "    return image\n",
        "\n",
        "class AnalyzeRequest(BaseModel):\n",
        "    prompt: str\n",
        "    image_base64: str\n",
        "    max_tokens: int = 512\n",
        "\n",
        "def decode_base64_image(data_url: str) -> bytes:\n",
        "    if \",\" in data_url:\n",
        "        data_url = data_url.split(\",\", 1)[1]\n",
        "    return base64.b64decode(data_url)\n",
        "\n",
        "app = FastAPI(title=\"ClinIQ ‚Äì MedGemma API\")\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a clinical decision support assistant.\n",
        "\n",
        "Rules:\n",
        "- Do NOT provide diagnoses\n",
        "- Use observational language only\n",
        "- Explicitly state uncertainty\n",
        "- Phrase findings for clinicians\n",
        "- Avoid prescriptive advice\n",
        "\n",
        "Respond ONLY with valid JSON.\n",
        "\n",
        "JSON schema:\n",
        "{\n",
        "  \"observations\": [],\n",
        "  \"possible_interpretations\": [],\n",
        "  \"uncertainty_notes\": \"\",\n",
        "  \"recommend_next_steps\": []\n",
        "}\n",
        "\"\"\"\n",
        "@app.post(\"/debug\")\n",
        "\n",
        "def run_medgemma(image, prompt, max_tokens):\n",
        "    # 1Ô∏è‚É£ Build Gemma-3 multimodal messages\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # 2Ô∏è‚É£ Apply Gemma chat template (THIS inserts image tokens correctly)\n",
        "    formatted_prompt = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    print(\"===== GEMMA PROMPT DEBUG =====\")\n",
        "    print(formatted_prompt)\n",
        "    print(\"================================\")\n",
        "\n",
        "    # 3Ô∏è‚É£ Processor call (now image tokens EXIST)\n",
        "    inputs = processor(\n",
        "        text=formatted_prompt,\n",
        "        images=[image],\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # 4Ô∏è‚É£ Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "def analyze(req: AnalyzeRequest):\n",
        "    try:\n",
        "        image_bytes = decode_base64_image(req.image_base64)\n",
        "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "        result = run_medgemma(\n",
        "            image=image,\n",
        "            prompt=req.prompt,\n",
        "            max_tokens=req.max_tokens\n",
        "        )\n",
        "\n",
        "        return {\"response\": result}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå ANALYZE FAILED\")\n",
        "        traceback.print_exc()\n",
        "        raise HTTPException(status_code=422, detail=str(e))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3M0Hyl3pY4X"
      },
      "source": [
        "## Step 6: Run FastAPI server\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qunAkiKspY4X"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import uvicorn\n",
        "from threading import Thread\n",
        "\n",
        "# -----------------------\n",
        "# Logging setup\n",
        "# -----------------------\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(\"cliniq\")\n",
        "\n",
        "def start_api():\n",
        "    logger.info(\"Starting FastAPI server on 127.0.0.1:8000\")\n",
        "\n",
        "    uvicorn.run(\n",
        "        app,\n",
        "        host=\"127.0.0.1\",\n",
        "        port=8000,\n",
        "        log_level=\"info\",\n",
        "        access_log=True\n",
        "    )\n",
        "\n",
        "    logger.info(\"Uvicorn process exited\")\n",
        "\n",
        "Thread(target=start_api).start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-4LriNOpY4X"
      },
      "source": [
        "## Step 7 Expose via Cloudflare Tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UterxS4WpY4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80b62a6-3346-4763-8566-e60bb587861a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-06T07:41:45Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2026-02-06T07:41:45Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "\n",
            "üåç COPY THIS URL ‚Üë‚Üë‚Üë\n",
            "\n",
            "2026-02-06T07:41:48Z INF +--------------------------------------------------------------------------------------------+\n",
            "2026-02-06T07:41:48Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2026-02-06T07:41:48Z INF |  https://divided-valves-shopper-cancer.trycloudflare.com                                   |\n",
            "\n",
            "üåç COPY THIS URL ‚Üë‚Üë‚Üë\n",
            "\n",
            "2026-02-06T07:41:48Z INF +--------------------------------------------------------------------------------------------+\n",
            "2026-02-06T07:41:48Z INF Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "2026-02-06T07:41:48Z INF Version 2026.1.2 (Checksum e157c54e929cc289cbd53860453168c2fe3439eb55e2e965a56579252585d9c1)\n",
            "2026-02-06T07:41:48Z INF GOOS: linux, GOVersion: go1.24.11, GoArch: amd64\n",
            "2026-02-06T07:41:48Z INF Settings: map[ha-connections:1 no-autoupdate:true p:http2 protocol:http2 url:http://127.0.0.1:8000]\n",
            "2026-02-06T07:41:48Z INF Generated Connector ID: d81f7e0d-ff64-451f-979f-37ae486da336\n",
            "2026-02-06T07:41:48Z INF Initial protocol http2\n",
            "2026-02-06T07:41:48Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2026-02-06T07:41:48Z INF ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "2026-02-06T07:41:48Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2026-02-06T07:41:48Z INF ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "2026-02-06T07:41:48Z INF Starting metrics server on 127.0.0.1:20241/metrics\n",
            "2026-02-06T07:41:48Z INF Registered tunnel connection connIndex=0 connection=e84af5a7-7fd2-463e-b11b-709187b48dcb event=0 ip=198.41.192.27 location=sea01 protocol=http2\n",
            "===== GEMMA PROMPT DEBUG =====\n",
            "<bos><start_of_turn>user\n",
            "<start_of_image><image>\n",
            "You are a clinical decision support assistant.\n",
            "\n",
            "Rules:\n",
            "- Do NOT provide diagnoses\n",
            "- Use observational language only\n",
            "- Explicitly state uncertainty\n",
            "- Phrase findings for clinicians\n",
            "- Avoid prescriptive advice\n",
            "\n",
            "Respond ONLY with valid JSON as plain text.\n",
            "\n",
            "JSON schema:\n",
            "{\n",
            "  \"observations\": [],\n",
            "  \"possible_interpretations\": [],\n",
            "  \"uncertainty_notes\": \"\",\n",
            "  \"recommend_next_steps\": []\n",
            "}\n",
            "\n",
            "Clinical question:\n",
            "Describe observable findings in this image.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            "================================\n",
            "2026-02-06T07:45:50Z ERR  error=\"Incoming request ended abruptly: context canceled\" connIndex=0 event=1 ingressRule=0 originService=http://127.0.0.1:8000\n",
            "2026-02-06T07:45:50Z ERR failed to serve incoming request error=\"Failed to proxy HTTP: Incoming request ended abruptly: context canceled\"\n",
            "INFO:     34.122.147.229:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "===== GEMMA PROMPT DEBUG =====\n",
            "<bos><start_of_turn>user\n",
            "<start_of_image><image>\n",
            "You are a clinical decision support assistant.\n",
            "\n",
            "Rules:\n",
            "- Do NOT provide diagnoses\n",
            "- Use observational language only\n",
            "- Explicitly state uncertainty\n",
            "- Phrase findings for clinicians\n",
            "- Avoid prescriptive advice\n",
            "\n",
            "Respond ONLY with valid JSON as plain text.\n",
            "\n",
            "JSON schema:\n",
            "{\n",
            "  \"observations\": [],\n",
            "  \"possible_interpretations\": [],\n",
            "  \"uncertainty_notes\": \"\",\n",
            "  \"recommend_next_steps\": []\n",
            "}\n",
            "\n",
            "Clinical question:\n",
            "Describe observable findings in this image.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            "================================\n",
            "INFO:     205.169.39.8:0 - \"GET / HTTP/1.1\" 404 Not Found\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import re\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    [\n",
        "        \"./cloudflared-linux-amd64\",\n",
        "        \"tunnel\",\n",
        "        \"--no-autoupdate\",\n",
        "        \"--protocol\", \"http2\",        # ‚ùå no QUIC\n",
        "        \"--url\", \"http://127.0.0.1:8000\"\n",
        "    ],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end=\"\")\n",
        "    if \"trycloudflare.com\" in line:\n",
        "        print(\"\\nüåç COPY THIS URL ‚Üë‚Üë‚Üë\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "431f2487eeb74f97836d69cf20c96c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46dccbc89bdf45cf887ee32e4dc3700b",
              "IPY_MODEL_62a3ddec75154d2098465a6ef602688f",
              "IPY_MODEL_5b5a7f7c46ce423d9db58c6d197e23aa"
            ],
            "layout": "IPY_MODEL_c0658eb476f849f79c9e71cf1a2653c3"
          }
        },
        "46dccbc89bdf45cf887ee32e4dc3700b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_689e112d2c984bfe878daa5d4ceca102",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a7d1e2e00f14287bc7e8f9f9b4a6473",
            "value": "Loading‚Äáweights:‚Äá100%"
          }
        },
        "62a3ddec75154d2098465a6ef602688f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bbb1c054b264bc292d683ac3c7390d4",
            "max": 883,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc1d5a6174a142cbad45ea9dd68a9628",
            "value": 883
          }
        },
        "5b5a7f7c46ce423d9db58c6d197e23aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e45926cb57547c4ab49962eae23586c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_707cba7658904aa29912fca9f7e78452",
            "value": "‚Äá883/883‚Äá[00:39&lt;00:00,‚Äá98.03it/s,‚ÄáMaterializing‚Äáparam=model.vision_tower.vision_model.post_layernorm.weight]"
          }
        },
        "c0658eb476f849f79c9e71cf1a2653c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "689e112d2c984bfe878daa5d4ceca102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a7d1e2e00f14287bc7e8f9f9b4a6473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bbb1c054b264bc292d683ac3c7390d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc1d5a6174a142cbad45ea9dd68a9628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e45926cb57547c4ab49962eae23586c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707cba7658904aa29912fca9f7e78452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}