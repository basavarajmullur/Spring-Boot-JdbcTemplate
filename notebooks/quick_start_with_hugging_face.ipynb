{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fgVWTMK9SNz"
      },
      "source": [
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q fastapi uvicorn pyngrok pillow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xt2XZgaaH2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you'll need to have a runtime with [sufficient resources](https://ai.google.dev/gemma/docs/core#sizes) to run the MedGemma model.\n",
        "\n",
        "You can try out MedGemma 4B for free in Google Colab using a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **‚ñæ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "**Note**: To run the demo with MedGemma 27B in Google Colab, you will need a runtime with an A100 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ITcQtdal7J"
      },
      "source": [
        "### Get access to MedGemma\n",
        "\n",
        "Before you get started, make sure that you have access to MedGemma models on Hugging Face:\n",
        "\n",
        "1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).\n",
        "2. Head over to the [MedGemma model page](https://huggingface.co/google/medgemma-1.5-4b-it) and accept the usage conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFQnPL2a9Dj"
      },
      "source": [
        "### Step 1: Authenticate with Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZwUUIY0gpY4W"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7xTbWg6pY4W"
      },
      "source": [
        "### Step 2: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CulOXOrhpY4W"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "  fastapi \\\n",
        "  uvicorn \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  pillow==10.4.0 \\\n",
        "  torch torchvision \\\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRN9Yg_kpY4X"
      },
      "source": [
        "## Step 3: Load MedGemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YORs_sDfpY4X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "\n",
        "MODEL_ID = \"google/medgemma-4b-it\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ MedGemma loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPhEFjiOTpcM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2Of7LKuT_Sz"
      },
      "source": [
        "## Step 4: Install cloudflared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dh1QcEXJT8zj"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9F5HEr6UMqO"
      },
      "source": [
        "## Step 5: MEDICAL SYSTEM PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjGwhqdfUVI0"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, UploadFile, Form\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "import base64\n",
        "from fastapi import HTTPException\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import traceback\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "def base64_to_pil(image_base64: str) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Convert a base64 string (raw or data URL) into a PIL Image.\n",
        "\n",
        "    Supports:\n",
        "    - Raw base64 (no prefix)\n",
        "    - data:image/png;base64,...\n",
        "    - data:image/jpeg;base64,...\n",
        "    \"\"\"\n",
        "\n",
        "    if not image_base64:\n",
        "        raise ValueError(\"Empty base64 image string\")\n",
        "\n",
        "    # Strip data URL prefix if present\n",
        "    if image_base64.startswith(\"data:\"):\n",
        "        image_base64 = image_base64.split(\",\", 1)[1]\n",
        "\n",
        "    try:\n",
        "        image_bytes = base64.b64decode(image_base64, validate=True)\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"Invalid base64 image data\") from e\n",
        "\n",
        "    try:\n",
        "        image = Image.open(io.BytesIO(image_bytes))\n",
        "        image = image.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"Decoded bytes are not a valid image\") from e\n",
        "\n",
        "    return image\n",
        "\n",
        "class AnalyzeRequest(BaseModel):\n",
        "    prompt: str\n",
        "    image_base64: str\n",
        "    max_tokens: int = 512\n",
        "\n",
        "def decode_base64_image(data_url: str) -> bytes:\n",
        "    if \",\" in data_url:\n",
        "        data_url = data_url.split(\",\", 1)[1]\n",
        "    return base64.b64decode(data_url)\n",
        "\n",
        "app = FastAPI(title=\"ClinIQ ‚Äì MedGemma API\")\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a clinical decision support assistant.\n",
        "\n",
        "Rules:\n",
        "- Do NOT provide diagnoses\n",
        "- Use observational language only\n",
        "- Explicitly state uncertainty\n",
        "- Phrase findings for clinicians\n",
        "- Avoid prescriptive advice\n",
        "\n",
        "Respond ONLY with valid JSON.\n",
        "\n",
        "JSON schema:\n",
        "{\n",
        "  \"observations\": [],\n",
        "  \"possible_interpretations\": [],\n",
        "  \"uncertainty_notes\": \"\",\n",
        "  \"recommend_next_steps\": []\n",
        "}\n",
        "\"\"\"\n",
        "@app.post(\"/debug\")\n",
        "\n",
        "def run_medgemma(image, prompt, max_tokens):\n",
        "    # üîí Force exactly ONE image token\n",
        "    clean_prompt = prompt.replace(\"<image>\", \"\").strip()\n",
        "    final_prompt = f\"<image>\\n{clean_prompt}\"\n",
        "\n",
        "    # üîç HARD DEBUG (DO NOT REMOVE YET)\n",
        "    print(\"===== GEMMA PROMPT DEBUG =====\")\n",
        "    print(repr(final_prompt))\n",
        "    print(\"Contains <image>:\", \"<image>\" in final_prompt)\n",
        "    print(\"Image type:\", type(image))\n",
        "    print(\"================================\")\n",
        "\n",
        "    # üö® Absolute safety check\n",
        "    if \"<image>\" not in final_prompt:\n",
        "        raise ValueError(\"FATAL: <image> token missing before processor\")\n",
        "\n",
        "    inputs = processor(\n",
        "        text=final_prompt,\n",
        "        images=[image],          # üëà MUST be a list\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "def analyze(req: AnalyzeRequest):\n",
        "    try:\n",
        "        image_bytes = decode_base64_image(req.image_base64)\n",
        "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "        result = run_medgemma(\n",
        "            image=image,\n",
        "            prompt=req.prompt,\n",
        "            max_tokens=req.max_tokens\n",
        "        )\n",
        "\n",
        "        return {\"response\": result}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå ANALYZE FAILED\")\n",
        "        traceback.print_exc()\n",
        "        raise HTTPException(status_code=422, detail=str(e))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3M0Hyl3pY4X"
      },
      "source": [
        "## Step 6: Run FastAPI server\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qunAkiKspY4X"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import uvicorn\n",
        "from threading import Thread\n",
        "\n",
        "# -----------------------\n",
        "# Logging setup\n",
        "# -----------------------\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(\"cliniq\")\n",
        "\n",
        "def start_api():\n",
        "    logger.info(\"Starting FastAPI server on 127.0.0.1:8000\")\n",
        "\n",
        "    uvicorn.run(\n",
        "        app,\n",
        "        host=\"127.0.0.1\",\n",
        "        port=8000,\n",
        "        log_level=\"info\",\n",
        "        access_log=True\n",
        "    )\n",
        "\n",
        "    logger.info(\"Uvicorn process exited\")\n",
        "\n",
        "Thread(target=start_api).start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-4LriNOpY4X"
      },
      "source": [
        "## Step 7 Expose via Cloudflare Tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UterxS4WpY4X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "a451d2c3-a00b-4e2f-a36a-f0db5b608c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-05T15:53:43Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2026-02-05T15:53:43Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "\n",
            "üåç COPY THIS URL ‚Üë‚Üë‚Üë\n",
            "\n",
            "2026-02-05T15:53:45Z INF +--------------------------------------------------------------------------------------------+\n",
            "2026-02-05T15:53:45Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2026-02-05T15:53:45Z INF |  https://baltimore-webmasters-nothing-terrorist.trycloudflare.com                          |\n",
            "\n",
            "üåç COPY THIS URL ‚Üë‚Üë‚Üë\n",
            "\n",
            "2026-02-05T15:53:45Z INF +--------------------------------------------------------------------------------------------+\n",
            "2026-02-05T15:53:45Z INF Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "2026-02-05T15:53:45Z INF Version 2026.1.2 (Checksum e157c54e929cc289cbd53860453168c2fe3439eb55e2e965a56579252585d9c1)\n",
            "2026-02-05T15:53:45Z INF GOOS: linux, GOVersion: go1.24.11, GoArch: amd64\n",
            "2026-02-05T15:53:45Z INF Settings: map[ha-connections:1 no-autoupdate:true p:http2 protocol:http2 url:http://127.0.0.1:8000]\n",
            "2026-02-05T15:53:45Z INF Generated Connector ID: 954ff749-dde6-45ab-a0b9-62f56ecc00a4\n",
            "2026-02-05T15:53:45Z INF Initial protocol http2\n",
            "2026-02-05T15:53:45Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2026-02-05T15:53:45Z INF ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "2026-02-05T15:53:45Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2026-02-05T15:53:45Z INF ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "2026-02-05T15:53:45Z INF Starting metrics server on 127.0.0.1:20241/metrics\n",
            "2026-02-05T15:53:45Z INF Registered tunnel connection connIndex=0 connection=ecb1f50b-7ca0-4d1c-a9da-a857539568a2 event=0 ip=198.41.192.107 location=sea01 protocol=http2\n",
            "‚ùå ANALYZE FAILED\n",
            "INFO:     49.207.147.92:0 - \"POST /analyze HTTP/1.1\" 422 Unprocessable Entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-970128221.py\", line 111, in analyze\n",
            "    result = run_medgemma(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-970128221.py\", line 87, in run_medgemma\n",
            "    inputs = processor(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/processing_gemma3.py\", line 109, in __call__\n",
            "    raise ValueError(\n",
            "ValueError: Prompt contained 0 image tokens but received 1 images.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2620496088.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"trycloudflare.com\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import re\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    [\n",
        "        \"./cloudflared-linux-amd64\",\n",
        "        \"tunnel\",\n",
        "        \"--no-autoupdate\",\n",
        "        \"--protocol\", \"http2\",        # ‚ùå no QUIC\n",
        "        \"--url\", \"http://127.0.0.1:8000\"\n",
        "    ],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end=\"\")\n",
        "    if \"trycloudflare.com\" in line:\n",
        "        print(\"\\nüåç COPY THIS URL ‚Üë‚Üë‚Üë\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}