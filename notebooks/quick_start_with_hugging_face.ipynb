{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basavarajmullur/Spring-Boot-JdbcTemplate/blob/master/notebooks/quick_start_with_hugging_face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fgVWTMK9SNz"
      },
      "source": [
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q fastapi uvicorn pyngrok pillow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xt2XZgaaH2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you'll need to have a runtime with [sufficient resources](https://ai.google.dev/gemma/docs/core#sizes) to run the MedGemma model.\n",
        "\n",
        "You can try out MedGemma 4B for free in Google Colab using a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **‚ñæ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "**Note**: To run the demo with MedGemma 27B in Google Colab, you will need a runtime with an A100 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ITcQtdal7J"
      },
      "source": [
        "### Get access to MedGemma\n",
        "\n",
        "Before you get started, make sure that you have access to MedGemma models on Hugging Face:\n",
        "\n",
        "1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).\n",
        "2. Head over to the [MedGemma model page](https://huggingface.co/google/medgemma-1.5-4b-it) and accept the usage conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFQnPL2a9Dj"
      },
      "source": [
        "### Step 1: Authenticate with Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwUUIY0gpY4W"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7xTbWg6pY4W"
      },
      "source": [
        "### Step 2: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CulOXOrhpY4W"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "  fastapi \\\n",
        "  uvicorn \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  pillow==10.4.0 \\\n",
        "  torch torchvision \\\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRN9Yg_kpY4X"
      },
      "source": [
        "## Step 3: Load MedGemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YORs_sDfpY4X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "\n",
        "MODEL_ID = \"google/medgemma-4b-it\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ MedGemma loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPhEFjiOTpcM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2Of7LKuT_Sz"
      },
      "source": [
        "## Step 4: Install cloudflared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh1QcEXJT8zj"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9F5HEr6UMqO"
      },
      "source": [
        "## Step 5: MEDICAL SYSTEM PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjGwhqdfUVI0"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, UploadFile, Form\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "import base64\n",
        "from fastapi import HTTPException\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import traceback\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "def base64_to_pil(image_base64: str) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Convert a base64 string (raw or data URL) into a PIL Image.\n",
        "\n",
        "    Supports:\n",
        "    - Raw base64 (no prefix)\n",
        "    - data:image/png;base64,...\n",
        "    - data:image/jpeg;base64,...\n",
        "    \"\"\"\n",
        "\n",
        "    if not image_base64:\n",
        "        raise ValueError(\"Empty base64 image string\")\n",
        "\n",
        "    # Strip data URL prefix if present\n",
        "    if image_base64.startswith(\"data:\"):\n",
        "        image_base64 = image_base64.split(\",\", 1)[1]\n",
        "\n",
        "    try:\n",
        "        image_bytes = base64.b64decode(image_base64, validate=True)\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"Invalid base64 image data\") from e\n",
        "\n",
        "    try:\n",
        "        image = Image.open(io.BytesIO(image_bytes))\n",
        "        image = image.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"Decoded bytes are not a valid image\") from e\n",
        "\n",
        "    return image\n",
        "\n",
        "class AnalyzeRequest(BaseModel):\n",
        "    prompt: str\n",
        "    image_base64: str\n",
        "    max_tokens: int = 512\n",
        "\n",
        "def decode_base64_image(data_url: str) -> bytes:\n",
        "    if \",\" in data_url:\n",
        "        data_url = data_url.split(\",\", 1)[1]\n",
        "    return base64.b64decode(data_url)\n",
        "\n",
        "app = FastAPI(title=\"ClinIQ ‚Äì MedGemma API\")\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a clinical decision support assistant.\n",
        "\n",
        "Rules:\n",
        "- Do NOT provide diagnoses\n",
        "- Use observational language only\n",
        "- Explicitly state uncertainty\n",
        "- Phrase findings for clinicians\n",
        "- Avoid prescriptive advice\n",
        "\n",
        "Respond ONLY with valid JSON.\n",
        "\n",
        "JSON schema:\n",
        "{\n",
        "  \"observations\": [],\n",
        "  \"possible_interpretations\": [],\n",
        "  \"uncertainty_notes\": \"\",\n",
        "  \"recommend_next_steps\": []\n",
        "}\n",
        "\"\"\"\n",
        "@app.post(\"/debug\")\n",
        "\n",
        "def run_medgemma(image, prompt, max_tokens):\n",
        "    # üîí Force exactly ONE image token\n",
        "    clean_prompt = prompt.replace(\"<image>\", \"\").strip()\n",
        "    final_prompt = f\"<image>\\n{clean_prompt}\"\n",
        "\n",
        "    # üîç HARD DEBUG (DO NOT REMOVE YET)\n",
        "    print(\"===== GEMMA PROMPT DEBUG =====\")\n",
        "    print(repr(final_prompt))\n",
        "    print(\"Contains <image>:\", \"<image>\" in final_prompt)\n",
        "    print(\"Image type:\", type(image))\n",
        "    print(\"================================\")\n",
        "\n",
        "    # üö® Absolute safety check\n",
        "    if \"<image>\" not in final_prompt:\n",
        "        raise ValueError(\"FATAL: <image> token missing before processor\")\n",
        "\n",
        "    inputs = processor(\n",
        "        text=final_prompt,\n",
        "        images=[image],          # üëà MUST be a list\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "def analyze(req: AnalyzeRequest):\n",
        "    try:\n",
        "        image_bytes = decode_base64_image(req.image_base64)\n",
        "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "        result = run_medgemma(\n",
        "            image=image,\n",
        "            prompt=req.prompt,\n",
        "            max_tokens=req.max_tokens\n",
        "        )\n",
        "\n",
        "        return {\"response\": result}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå ANALYZE FAILED\")\n",
        "        traceback.print_exc()\n",
        "        raise HTTPException(status_code=422, detail=str(e))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3M0Hyl3pY4X"
      },
      "source": [
        "## Step 6: Run FastAPI server\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qunAkiKspY4X"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import uvicorn\n",
        "from threading import Thread\n",
        "\n",
        "# -----------------------\n",
        "# Logging setup\n",
        "# -----------------------\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(\"cliniq\")\n",
        "\n",
        "def start_api():\n",
        "    logger.info(\"Starting FastAPI server on 127.0.0.1:8000\")\n",
        "\n",
        "    uvicorn.run(\n",
        "        app,\n",
        "        host=\"127.0.0.1\",\n",
        "        port=8000,\n",
        "        log_level=\"info\",\n",
        "        access_log=True\n",
        "    )\n",
        "\n",
        "    logger.info(\"Uvicorn process exited\")\n",
        "\n",
        "Thread(target=start_api).start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-4LriNOpY4X"
      },
      "source": [
        "## Step 7 Expose via Cloudflare Tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UterxS4WpY4X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "3ed322bd-c7f0-48b0-aa83-0b7a5685cc2b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2620496088.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"trycloudflare.com\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import re\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    [\n",
        "        \"./cloudflared-linux-amd64\",\n",
        "        \"tunnel\",\n",
        "        \"--no-autoupdate\",\n",
        "        \"--protocol\", \"http2\",        # ‚ùå no QUIC\n",
        "        \"--url\", \"http://127.0.0.1:8000\"\n",
        "    ],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end=\"\")\n",
        "    if \"trycloudflare.com\" in line:\n",
        "        print(\"\\nüåç COPY THIS URL ‚Üë‚Üë‚Üë\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}